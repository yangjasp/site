[
  {
    "objectID": "posts/luck_in_sports1/index.html",
    "href": "posts/luck_in_sports1/index.html",
    "title": "May the Luckiest Team Win",
    "section": "",
    "text": "The Los Angeles Dodgers, indisputably the best team in the 2022 Major League Baseball regular season, were eliminated from World Series contention last night after narrowly falling in a best-of-five series against the San Diego Padres. Although they defeated the Padres in 14 out of their 19 regular season matchups, many fans will look back on this year regarding the Dodgers as the inferior team, and their 2022 season is already being called an utter dissapointment. This is of course not the first time that an American sports team’s legacy has been tarnished by a surprising playoff elimination. The 2014-15 Golden State Warriors, arguably the greatest NBA team of all time, famously blew a 3-1 lead to the Cleveland Cavaliers in the NBA finals, leading to pundits questioning their greatness and forcing me to use “arguably” in this sentence. The 2001 Seattle Mariners won the most regular season games in MLB history but failed to reach the World Series. The 2008 New England Patriots became only the second NFL team ever to finish the regular season undefeated only to lose to the six-loss New York Giants in the Super Bowl. This list could go on and on.\nUnsurprisingly, each of these playoff eliminations has been quickly followed by hindsight explanations of why the team was doomed to fail. Sportswriters are already pointing fingers at the Dodger’s GM and manager for poor strategic decisions, and a quick Twitter search will reveal thousands of angry fans calling out the players for being “chokers” unable to handle the pressure of the postseason. While these criticisms may be warranted to a small extent, the main culprit for postseason upsets is something frequently pushed to distant background of discussions: bad luck."
  },
  {
    "objectID": "posts/luck_in_sports1/index.html#defining-luck",
    "href": "posts/luck_in_sports1/index.html#defining-luck",
    "title": "May the Luckiest Team Win",
    "section": "Defining Luck",
    "text": "Defining Luck\nLuck is generally a widely understood term, but coming up with a technical definition is actually quite challenging and philosophical. The approach that resonates most with me is to first define skill as ability (including knowledge, physical traits, preparedness, etc.) that is applied to an activity by a participant, and then call everything else that influences the outcome luck. Put more simply, luck is everything outside of the participant’s control. Take, for example, a simple game where a participant randomly picks a marble out of a bag containing 4 red marbles and 1 blue marble and wins 1 dollar if they correctly predicted the color of the selected marble. Here, the skill is understanding that choosing red is the optimal winning strategy, but whether or not the participant actually wins ultimately boils down to luck1.\nSome of the references that I mention later in this post provide their own definitions of luck or avoid addressing the issue altogether, but I believe that establishing a rough definition is a necessary starting point for this discussion."
  },
  {
    "objectID": "posts/luck_in_sports1/index.html#skill-vs.-luck-in-sports",
    "href": "posts/luck_in_sports1/index.html#skill-vs.-luck-in-sports",
    "title": "May the Luckiest Team Win",
    "section": "Skill vs. Luck in sports",
    "text": "Skill vs. Luck in sports\nThere is no disputing the fact that it takes an incredible amount of skill to play a sport at a professional level. There is also no disputing the fact that a team of professional baseball players would beat my local recreational league team well more than 99.999% of the time. But because the MLB only features similarly high-skilled players, the skill gap is actually relatively small. When the skill gap is small, luck becomes a much more influential factor on the outcome.\nTo see how this works, take a simple example of free throws in basketball. According to Statmuse, Stephen Curry has made 90.8% of his free throws in his career. Thus, if we consider each shot as independent, it is reasonable to estimate that he has a 90.8% chance of making each free throw he takes. I am significantly less skilled at free throws compared to Curry, so let’s suppose that I have a 60% chance of making a given free throw. If we were to play a game where the person who makes more free throws out of 10 wins (let’s say that we reset the scores and replay the game if it is a tie), Curry would win about 98% of the time. But if Curry played this game against Kyrie Irving, a similarly skilled free-throw shooter who has made 88.2% of attempts in his career, Curry would only win about 60% of the time. If Curry were to play against himself (or someone else equally skilled and mentally prepared etc.), even Bobby Knight would have to admit that the winner was crowned based purely on luck.\nFree throws are obviously an extreme example of a controlled situation where randomness plays a part, but every sport is made up of thousands of instances similar to this. Given the same ground ball, a baseball infielder only makes an error a small percentage of the time. Pitchers only hit the spot they were aiming for some of the time, and batters only get on base some of the time. In soccer, forwards only score some of the time whether it’s a deflected shot or a penalty kick. The same line of thought applies to teams as a whole. Luck on a team level is the sum of these luck-influenced event on the individual level of the team members plus even more events that arise from the interaction of individuals and game strategy. Even the best and most prepared center-back partnership might fail to be on the same page once during a game and that one slip up might happen at a crucial instance. A team manager might enact an optimal strategy that fails because of a referee decision. Randomness is everywhere.\nUltimately, the thousands of random events that make up every sporting competition result in better teams only beating worse teams some of the time. More skilled players and teams succeed in these events more often in the long run, but the differences between athletes (and teams) playing at the same level are small enough that luck plays a relatively large role in determining a competition’s winner. In baseball for example, the betting moneyline for the league-best Astros to beat the league-worst A’s at home this past August was -275. These odds, which have been shown to be very good predictors of the true long-run odds, roughly equate to an implied winning probability of 73% for the Astros. This essentially means that worst team in the MLB still has a 27% chance of beating the best team in a given game because of randomness. If the A’s train hard and prepare well (i.e. improve their skill level) and/or the Astros become more error-prone due to the pressure, the win probability might shift slightly in the A’s favor, but luck will always be in play."
  },
  {
    "objectID": "posts/luck_in_sports1/index.html#sports-on-the-luck-skill-spectrum",
    "href": "posts/luck_in_sports1/index.html#sports-on-the-luck-skill-spectrum",
    "title": "May the Luckiest Team Win",
    "section": "Sports on the Luck-Skill Spectrum",
    "text": "Sports on the Luck-Skill Spectrum\nIn his compelling book on this subject, Michael Mauboussin writes in detail about the luck-skill spectrum. Gambling games like roulette and bingo are games of pure luck and chess at a high level is almost purely skill. Games like poker and blackjack lie in between but lean heavily towards luck.\nBy the same token, sports can be arranged on the spectrum too. This 2011 blog post written by Phil Birnbaum describes a really neat method to quantify the contribution of luck in sports leagues developed by well-known sabermetrician Tom Tango. Mauboussin writes about this method in his book as well. The underlying idea is that variance in team win-loss records at the end of a season are a sum of skill and luck:\n\\[\nvar(observed \\space records) = var(skill) + var(luck)\n\\]\nThe variance from the observed records is known. An estimate for \\(var(luck)\\) can be calculated as the theoretical variance that we would observe if each game was determined by a coin toss. More technically, this is the variance of a binomial distribution with \\(p = 0.5\\) and \\(n =\\) the number of games played. With two out of three terms in the above equation known, solving for \\(var(skill)\\) is simple. The ratio of \\(var(luck)\\) to \\(var(observed \\space records)\\) then reveals what portion of the final standings can be attributed to luck.\nUsing this method, Mauboussin ranks the NBA as the major American professional league least influence by luck and the NFL as the league most influenced by luck. This finding is largely influenced by the number of games in each league’s season. Luck plays a larger role in determining the outcome of individual MLB games compared to NFL games, but each NFL team plays 17 games in a season compared to 162 in the MLB. A larger sample size of games decreases the relative role of luck in the cumulative outcome in the same way that my probability of beating Steph Curry in the previously described free throw game is maximized (at about 13%) if the game consists of only one shot and falls below 0.01% if the game consists of 30 shots. Hence why playoff matchups, which range from best-of-one in the NFL to best-of-three in the MLB wild card round to best-of-seven in the MLB championship, are so influenced by luck2."
  },
  {
    "objectID": "posts/luck_in_sports1/index.html#the-clutch-factor",
    "href": "posts/luck_in_sports1/index.html#the-clutch-factor",
    "title": "May the Luckiest Team Win",
    "section": "The Clutch Factor",
    "text": "The Clutch Factor\nPerformance in pressure situations including playoffs is frequently associated with skill of mentality, otherwise known as “clutchness”. A great deal of research has provided evidence that pressure impacts athletes performance and that some athletes handle it better than others. Still, this doesn’t mean that Kobe Bryant’s game-tying three-pointer in Game 2 of the 2004 NBA finals went in simply because he has the “clutch gene” and performs well under pressure. All it means is that Kobe Bryant is skilled enough at basketball and composed enough to give himself a non-zero probability of making that shot. Then good luck did the rest. Can one made shot tell give us any confidence to conclude that Kobe has an innate ability to perform well under pressure? No. In fact, despite the narrative that many pundits like to push, his shooting statistics in high-pressure situations are actually quite poor. But these numbers don’t necesarilly mean that he’s a choker who can’t handle the pressure either - they might just be a product of bad luck (or other factors such as increased double teams by opponents expecting him too shoot). We’ll never know."
  },
  {
    "objectID": "posts/luck_in_sports1/index.html#luck-is-a-boring-narrative-and-playoffs-are-fun-to-watch.-im-okay-with-thatfor-the-most-part",
    "href": "posts/luck_in_sports1/index.html#luck-is-a-boring-narrative-and-playoffs-are-fun-to-watch.-im-okay-with-thatfor-the-most-part",
    "title": "May the Luckiest Team Win",
    "section": "Luck is a boring narrative and playoffs are fun to watch. I’m okay with that…for the most part",
    "text": "Luck is a boring narrative and playoffs are fun to watch. I’m okay with that…for the most part\nThe sporting industry is one of the largest global sources of money and entertainment. In the modern age of TV and social media, the stories, predictions, and personalities surrounding sports are as central to their popularity as competetive events themselves. The purpose of sports is to entertain people, and luck-driven uncertainty and underdog stories contribute heavily to the entertainment value that sports provide. Of course, skill also plays a massive role, and it is hard to blame the media for focusing on the talent, brilliance, and hard work that led to success (well, increased probability of success) rather than the luck it required.\nWith that said, competitions that practically mirror coin-flips and then go on to claim that champions are deserving of their crown because of their skill should be frustrating for everyone involved. This is essentially what the playoffs are. Nearly every major men’s and women’s sports league in the U.S. uses a a playoff system to crown the league champion, and in nearly half, exactly half, or in the WNBA’s case more than half of the teams that participated in the long, grueling regular season qualify for these playoffs. As soon as the playoffs start, the only thing that the regular season counts for is homefield advantage (which only slightly benefits the home team]). The team that demonstrated its superiority over the large sample size of the regular season sees hardly any reward for it come playoff time, and as the 2022 Dodgers showed us, their reputation of truly being the best team relies on having enough luck to win a best-of-five series followed by a best-of-seven series followed by another best-of-seven series. Even a team with a 70% chance of winning each game (think league-best Astros vs. league-worst A’s every single series) will prevail in that format less than 64% of the time.\nFrustratingly, the amount of teams that make the playoffs has only been increasing in many leagues over recent years. Owners support this because adding playoff teams means adding playoff games, thereby increasing TV and ticket revenue. I find it extremely frustrating because it decreases the chances of deserving Champions. As some others have pointed out, expanded playoff systems also harm competitions by decreasing the incentive for teams to invest in better (more skilled) rosters. On the other hand, though, European soccer leagues don’t use playoffs at all, and as a result there is (sometimes) significantly less uncertainty and excitement surrounding the title race. Ultimately, there is no format that perfectly balances luck and skill, so we will have to settle for what we’ve got."
  },
  {
    "objectID": "posts/luck_in_sports1/index.html#footnotes",
    "href": "posts/luck_in_sports1/index.html#footnotes",
    "title": "May the Luckiest Team Win",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf the player chooses red but loses because the blue marble is picked, we would clearly call them “unlucky”. But what if they choose red (the optimal strategy) and they win because red is drawn. Did they win because they were skillfull? Or did they get lucky? Both, of course!↩︎\nOne really interesting observation that Mauboussin makes is that the influence of luck at the highest level of sports seems to be increasing with time across all leagues. The hypothesis that the improved access to extremely optimized training is pushing skill levels towards convergence at the peak of what is possible for humans is certainly intruiging.↩︎"
  },
  {
    "objectID": "posts/probable_sequences/index.html",
    "href": "posts/probable_sequences/index.html",
    "title": "Rediscovering a fun probability problem",
    "section": "",
    "text": "On a recent trip, two friends and I accidentally rediscovered a neat probability factoid while playing cards:\nThis “paradox” does not have a specific name as far as we can tell, but it is solidly documented and applies to many problems involving sequences of events. We had a lot of fun discovering it nonetheless, so I decided to write about thinking process that led us to it."
  },
  {
    "objectID": "posts/probable_sequences/index.html#arriving-at-the-problem",
    "href": "posts/probable_sequences/index.html#arriving-at-the-problem",
    "title": "Rediscovering a fun probability problem",
    "section": "Arriving at the problem",
    "text": "Arriving at the problem\nDuring a game of Dirty Clubs on a Northern Rail train from London to Liverpool, two friends and I came up with a seemingly straightforward question pertaining to a debate about optimal strategy: what is the expected number of cards until the sequence “Ace, Jack, Jack” appears in a with-replacement deck 1. To make things even simpler, we started by only considering draws from an eight-card deck of Jacks and Aces.\nOur initial approach was to draw a tree diagram to a reasonable number of draws and look for a pattern. To make things as simple as possible, we decided to consider AAA (Ace, Ace, Ace) rather than AJJ (Ace, Jack, Jack), knowing that the probability of flipping each of these sequences at a given time is equally \\(\\frac{1}{8}\\). Our initial attempts to write the expectation as an infinite sum were unsuccessful, so we eventually made use of my brief experience with stochastic processes and used the Markov chain expected waiting time strategy to arrive at an answer of 14 flips.\nSomething immediately felt off about that answer - 14 flips seemed far too high for a string of just three cards. We re-derived and re-solved the system of equations twice, but kept getting 14. In the end, we decided that it was the unsatisfactory, yet correct, answer.\nStill unsatisfied a few days later, we revisited the problem and realized that both our mathematical approach and our guts were half right, but neither was fully correct. The reason was extremely interesting."
  },
  {
    "objectID": "posts/probable_sequences/index.html#the-solution",
    "href": "posts/probable_sequences/index.html#the-solution",
    "title": "Rediscovering a fun probability problem",
    "section": "The Solution",
    "text": "The Solution\n\nA misconception of the “misconception of chance”\nIn a 1974 paper, Daniel Kahneman and Amos Tversky describe a fallacy called the “misconception of chance”, whereby people erroneously believe that sequences of independent events that appear random such as HTHTHHT (for a fair coin toss) are more likely than sequences that non-random such as HHHHHHH. In reality, independence tells us that the probability for each of those sequences is \\((\\frac{1}{2})^7\\), although the probability of getting the 4 heads out of 7 tosses is greater than the probability of getting 7 heads because there are more (equally likely) sequences that contain exactly 4 heads than there are sequences that contain 7 heads.\nIt was with this idea in mind that my friends and I thought that waiting for the sequence AAA was the same as waiting for the sequence AJJ. As we realized later, this fact is only true when the past/present are ignored. Two different viewpoints can be used to see why.\n\n\nViewpoint 1: Tree Diagrams\nTree diagrams are useful ways to visualize probabilities of (short) sequences of discrete events. It was my friend’s return to our diagram up to 4 card draws that first revealed the paradox. This diagram is pictured below:\n The key realization here is that the tree stops as soon as the sequence has been reached. Hence why the top A, which would be the fourth A in AAAA, is not circled in red. This A will never be reached if we are looking for AAA, so we have \\[P(\\text{reach AAA in 4 or less draws}) = P(\\text{reach AAA on 3rd}) + P(\\text{reach AAA on 4th}) = \\frac{1}{8} + \\frac{7}{8}\\cdot\\frac{1}{14} = \\frac{3}{16}.\\]\nOn the other hand, AJJ can’t occur after another AJJ within four draws. This gives\n\\[P(\\text{reach AJJ in 4 or less draws}) = P(\\text{reach AJJ on 3rd}) + P(\\text{reach AJJ in 4th}) = \\frac{1}{8} + \\frac{7}{8}\\cdot\\frac{2}{14} = \\frac{4}{16}.\\] Essentially, AAA is less likely because two of the chances to get AAA overlap. This is not the case for AJJ, so you actually have more chances in the first four draws to get AJJ. If you are looking for AJJ and the first card is an A but the second is also an A, which breaks the sequence, you still have another chance to get AJJ in four draws because the breaking card is actually the first in the sequence. If you are looking for AAA, a breaking Jack on the second draw eliminates any chance of getting the sequence in four or less.\n\n\nViewpoint 2: Stochastic Process\nA second approach to this problem is to consider a Markov chain with states corresponding to how far along the desired sequence you are:\n\nState 0: Starting state. Still need the full sequence of three, so the minimum number of turns left is three.\nState 1: The first card of the sequence has just been drawn. If the next two are drawn correctly, the game can end in two turns.\nState 2: The first two card of the sequence have just been drawn. If the next draw is what we are looking for, the game will end in one turn.\nState 3: The absorbing final state. This is reached when the desired sequence has just been drawn.\n\nUsing 0.5 as the probability for drawing A or J on each turn, the transition diagrams for AAA and AJJ are:\n\n Note the distinct differences. A break in the sequence will always knock you back to state 0 for AAA, but for AJJ, you never return to state 0 once you leave it because breaking the sequence after it has started only occurs when an A, the first card in the sequence, is drawn.\nThe expected number of draws until state 3, AAA, is reached can be found by solving the following system of equations, which follow directly from the diagram:\n\\[\nE_0 = \\frac{1}{2}(E_1 + 1) + \\frac{1}{2}(E_0 + 1)\n\\] \\[\nE_1 = \\frac{1}{2}(E_2 + 1) + \\frac{1}{2}(E_0 + 1)\n\\] \\[\nE_2 = \\frac{1}{2}(E_3 + 1) + \\frac{1}{2}(E_0 + 1)\n\\] where \\(E_i\\) is the expected time until state 3, AAA, is reached given that the chain is currently in state \\(i\\). Clearly \\(E_3 = 0\\), so this is a system of three equations with three unknowns. Solving for \\(E_0\\) gives 14.\nIn contrast, the system of equations for AJJ is:\n\\[\nE_0 = \\frac{1}{2}(E_1 + 1) + \\frac{1}{2}(E_0 + 1)\n\\] \\[\nE_1 = \\frac{1}{2}(E_2 + 1) + \\frac{1}{2}(E_1 + 1)\n\\] \\[\nE_2 = \\frac{1}{2}(E_3 + 1) + \\frac{1}{2}(E_1 + 1)\n\\]\nNow, solving for \\(E_0\\) gives 8. If you are looking for AJA, the Markov diagram and corresponding system of equations give 10 as the expected number of draws."
  },
  {
    "objectID": "posts/probable_sequences/index.html#now-we-know",
    "href": "posts/probable_sequences/index.html#now-we-know",
    "title": "Rediscovering a fun probability problem",
    "section": "Now we know…",
    "text": "Now we know…\nAfter working out these details, our assumption that AAA was obviously the same as AJJ felt like a silly mistake. As it turns out, a number of researchers have fallen victim to similar fallacies in the past, including Amos Tversky himself in a 1985 paper about the hot-hand in basketball. Probability is hard!"
  },
  {
    "objectID": "posts/probable_sequences/index.html#footnotes",
    "href": "posts/probable_sequences/index.html#footnotes",
    "title": "Rediscovering a fun probability problem",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs every Midwesterner should know, Ace, Jack, Jack is a good sequence of cards.↩︎"
  },
  {
    "objectID": "posts/vaccine_modelling/index.html",
    "href": "posts/vaccine_modelling/index.html",
    "title": "Modeling COVID-19 and Vaccine Rollout Strategies",
    "section": "",
    "text": "Over the past three weeks, the United States government has approved two different COVID-19 vaccines for emergency use. These two vaccines, one developed by Pfizer and the other by Moderna, are actually quite similar in that they are both two-dose mRNA vaccines with mild side-effects, and they have almost identical efficacy rates (95% and 94.1% respectively according to the FDA). The only notable difference is that the Moderna vaccine can be stored in a standard freezer while the Pfizer vaccine must be stored at -70 degrees Celsius, making it slightly less accessible for small healthcare centers.\nConsequently, U.S. vaccine rollout plans have so far treated the two vaccines as essentially being the same, that is to say neither of the two vaccines is considered better than the other. This somewhat unexpected fact has simplified the already controversial rollout, but the impending introductions of new vaccines from companies like Johnson & Johnson and AstraZaneca threaten to introduce a new complexity. If a new vaccine with only 70% efficacy becomes available, how should it be incorporated into the rollout? Is it better to give at-risk patients a worse vaccine if it means they can get vaccinated a month earlier? Will people even want it?\nThe answers to these questions are certainly complex, and they will likely draw from principles in biology, mathematics, statistics, and economics among other fields. I cannot pretend to be an expert here - I am hopeful that the true experts have been thinking deeply about this issue - but I think the problem is quite fascinating. This post contains my thoughts and some basic modeling!"
  },
  {
    "objectID": "posts/vaccine_modelling/index.html#what-is-vaccine-efficacy",
    "href": "posts/vaccine_modelling/index.html#what-is-vaccine-efficacy",
    "title": "Modeling COVID-19 and Vaccine Rollout Strategies",
    "section": "What is Vaccine Efficacy?",
    "text": "What is Vaccine Efficacy?\nBefore moving any further, let’s make sure we understand what vaccine efficacy is. Essentially, it is a measure of how well a vaccine prevents cases of the disease in a group of vaccinated subjects compared to a control group of un-vaccinated (received placebo) subjects. An estimate of the efficacy is generated from clinical trial data, which every vaccine must collect and submit to the FDA for review before being made available to the public.\nThe actual calculation of efficacy relies on a term called risk. In epidemiology, risk is the probability that an event (such as infection) will occur in a given time period, and it is measured in a clinical trial as the proportion of total subjects in a group who experienced the event in the time period that they were followed. For example, if 8 out of 1,000 patients who receive an actual vaccine in a clinical trial become infected with COVID-19, then the estimate of risk of infection in the vaccination group is simply 0.008, or 0.8%. The ratio of risk in the treatment (actual vaccine) group compared to the risk in the control (placebo) group is called the risk ratio. Risk ratios below 1 suggest that the treatment reduces the risk of disease compared to the control, and smaller ratios suggest a stronger reduction. Efficacy is calculated as 1 - Risk Ratio, so an efficacy closer to 1 (or 100%) means that the vaccine reduces the risk of infection by a greater proportion. You can read more about vaccine efficacy on the CDC website here.\nRemember that ratios are multiplicative, not additive, so we can intepret 95% vaccine efficacy (the equivalent to 100%-95% = 5% risk ratio) as meaning that the risk for COVID in the vaccine group is 0.05 (5%) times the risk in the control group."
  },
  {
    "objectID": "posts/vaccine_modelling/index.html#a-simple-model",
    "href": "posts/vaccine_modelling/index.html#a-simple-model",
    "title": "Modeling COVID-19 and Vaccine Rollout Strategies",
    "section": "A Simple Model",
    "text": "A Simple Model\nTo assess the potential effects of different vaccine rollouts, we will create a model of the COVID-19 pandemic in the United States. Our first step will be to develop a simple framework using principles from epidemiology before adding more complex features. For this simple model, we divide the population of the United States into four groups: susceptible (S), infected (I), recovered and immune (R), and deceased (D). At a specific point in time, every individual should fall into one and only one of these categories.\nWith these categories established, our next job is to determine a way to describe the flow of individuals from group to group. See Diagram 1 below:\n\n\n\n\n\n\n\n\nWe are interested in how the flow of individuals occurs over time, so we will measure it as the rate at which people exit/enter a group per day. Below we explore each arrow on the diagram above:\n\nS → I: The rate at which susceptible individuals become infected. This value intuitively depends on the rate of contact between susceptible and infected individuals as well as the probability that the disease would be transmitted given a contact occurs. Here we introduce our first parameter, \\(\\beta\\), which captures the rate of transmission. We can calculate \\(\\beta\\) using \\(R_0\\), the more intuitive “basic reproductive rate” of a disease, \\(\\gamma\\), the recovery rate (which is calculated as as 1/length of infectious period), and \\(N\\), the population size, because:\n\n\\[\\begin{align*}\n\\beta = \\frac{R_0 \\gamma}{N}\n\\end{align*}\\]\n       Thus, S → I is described by \\(\\beta S I\\) where \\(S\\) and \\(I\\) are the number\n       of individuals in groups S and I.\n\nI → D: The rate at which infected individuals die. This depends on our second parameter, \\(\\gamma\\), and a third parameter, \\(\\mu\\), the death rate. As the inverse of the infectious period, \\(\\gamma\\) captures the average number of infected individuals that stop being infectious each day (i.e. if the infectious period is 5 days, then 1/5 of infectious individuals stop being infectious each day). As the death rate, \\(\\mu\\) tells us what fraction of individuals leaving \\(I\\) are headed to \\(D\\) (what percent die), while the rest, or 1-\\(\\mu\\), recover and head to \\(R\\). Thus, I → D is described by \\(\\gamma I\\mu\\).\nI → R: The rate at which infected individuals recover. Using logic from I → D, above we have I → D is described by \\(\\gamma I(1-\\mu)\\). Note that the total rate at which individuals exit \\(I\\) is simply \\(\\gamma I\\) because every infected individual either dies or recovers.\nR → S: The rate at which recovered individuals lose immunity and become susceptible again. This value depends on a fourth paramter, \\(\\omega\\), the inverse of the period of immunity. It is the same idea as \\(\\gamma\\).\n\nWith these rates determined, we can now create a system of ordinary differential equations to describe the flow of people between groups. For each group, the change in size with respect to time (the derivative) is simply the sum of the rates with which people enter it (arrows entering the box) minus the rates with which people leave (arrows leaving the box). Thus, we have:\n\\[\\begin{align*}\n\\frac{dS}{dt} &= -\\beta S I + \\omega R \\\\\n\\frac{dI}{dt} &= \\beta S I - \\gamma I\\\\\n\\frac{dR}{dt} &= \\gamma I(1-\\mu) - \\omega R\\\\\n\\frac{dD}{dt} &= \\gamma I(\\mu)\n\\end{align*}\\]\nwhere the parameters \\(\\beta\\), \\(\\gamma\\), \\(\\omega\\), and \\(\\mu\\) are the transmission rate, recovery rate, loss of immunity rate, and death rate, respectively. If we set values for each of these parameters based on our knowledge of the COVID-19 pandemic and set initial sizes for each group, we can track the course of the disease by solving this system of ordinary differential equations for each day to model how the disease might progress."
  },
  {
    "objectID": "posts/vaccine_modelling/index.html#adding-long-term-immunity-vaccines",
    "href": "posts/vaccine_modelling/index.html#adding-long-term-immunity-vaccines",
    "title": "Modeling COVID-19 and Vaccine Rollout Strategies",
    "section": "Adding Long-Term Immunity: Vaccines",
    "text": "Adding Long-Term Immunity: Vaccines\nNow that we have established the basic theoretical framework of the model, we can start to add complexities such as vaccines. Vaccinated individuals will come out of the susceptible group and enter a new group with different properties from the four we established before. See Diagram 2 below:\n\n\n\n\n\n\n\n\nThis model assumes that every vaccinated individual is protected completely from infection, but we know that this is not the case for COVID-19 vaccines in the real world. Accounting for this fact and supposing that individuals can receive one of two different vaccines, we have a new model detailed in Diagram 3.\n\n\n\n\n\n\n\n\nAssuming that the number of vaccines given per day, \\(\\nu_1\\) and \\(\\nu_2\\), and the rates of success, \\(\\varphi_1\\) and \\(\\varphi_2\\), for each vaccine are known, we now have the following system of ordinary differential equations:\n\\[\\begin{align*}\n\\frac{dS}{dt} &= -\\beta S I + \\omega R -  \\nu_1  -  \\nu_2 \\\\\n\\frac{dV_{1}}{dt} &= \\nu_1 - (1-\\varphi_1) \\beta V_1 I\\\\\n\\frac{dV_{2}}{dt} &= \\nu_2 - (1-\\varphi_2) \\beta V_2 I \\\\\n\\frac{dI}{dt} &= \\beta S I - \\gamma I + (1-\\varphi_1)\\beta V_{1} I + (1-\\varphi_2)\\beta V_{2} I \\\\ &= \\beta I[S + (1-\\varphi_1)V_{1} + (1-\\varphi_2)V_{2}] - \\gamma I\\\\\n\\frac{dR}{dt} &= \\gamma I(1-\\mu) - \\omega R\\\\\n\\frac{dD}{dt} &= \\gamma I(\\mu)\n\\end{align*}\\]\nThere are obviously a number of assumptions that this model is making for the sake of simplicity. The big ones are:\n\nEvery individual has the same \\(\\beta\\), that is the same probability of coming into contact with an infected individual and then contracting it.\nActually, all the parameters \\(\\beta\\), \\(\\gamma\\), \\(\\omega\\), \\(\\mu\\), \\(\\nu_1\\), \\(\\nu_2\\), \\(\\varphi_1\\), and \\(\\varphi_2\\) are constant throughout the population and over time.\nThe population is closed other than COVID deaths. No births or deaths from other causes are accounted for.\n\nWe should be okay with most of these assumptions for the sake of our simple model. Accounting for the ways in which behavioral changes, weather patterns, and government policies change the parameters would help the model better reflect the real life course of the virus, but we will leave that to the experts. With that said, there is one key factor that is currently being ignored - age.\nIt is well known that virus impacts different age groups in very different ways. In terms of our parameters, the death rate, \\(\\mu\\), varies greatly across age groups. On top of that, the vaccine rollout plans (impacting \\(\\nu_1\\) and \\(\\nu_2\\)) will also certainly account for differences in age. In the following section, I discuss how we can incorporate age classes into our model."
  },
  {
    "objectID": "posts/vaccine_modelling/index.html#adding-age-classes",
    "href": "posts/vaccine_modelling/index.html#adding-age-classes",
    "title": "Modeling COVID-19 and Vaccine Rollout Strategies",
    "section": "Adding Age Classes",
    "text": "Adding Age Classes\nLet’s now suppose that our population is made up of distinct age classes where \\(N_i\\) is the number of individuals in age group \\(i\\). For our purposes, we will use the age classes (with their corresponding N and death rates) detailed below:\n\n\n\niAgePopulation (in Millions)Death Rate*10-419.580.0002625-920.190.00010310-1420.800.00010415-1921.060.00022520-2521.630.00040626-3023.500.00040731-3522.430.00135836-4021.730.00135941-4519.920.003461046-5020.400.003461151-5520.480.012671256-6021.870.012671361-6520.570.012671466-7017.460.048201571-7514.030.048201676-809.650.116361781-856.320.116361885+6.610.22083*Death Rate is calculated as overall deaths rate of population divided by overall incidence rate in population according to the CDC's ACIP report on 12/20/20. Population totals are for 2019 from Statistica.com.\n\n\nThe seemingly simple, however incorrect, approach would be to run our previously specified vaccine model on each age group individually and calculate the overall size of each group as the sum of its size in each age group. For example, \\(S = \\sum_{i=1}^{18} S_i\\). The problem with this approach is that it assumes that individuals only interact with, and therefore can only be infected by, people in the same age class. This is clearly not reflective of the real world.\nTo solve this issue, we need a contact matrix. A contact matrix describes the contacts between age group \\(i\\) and age group \\(j\\). As an example, let’s suppose that there are 900 students and 100 teachers in a school. Students make on average 40 contacts with other students and 10 contacts with teachers per day. Since the ratio of students to teachers is 9:1, students making 10 contacts with teachers on average means that teachers make on 10*9 = 90 contacts with students per day an average. Now also assume that the teachers only make contact with 5 other teachers per day on average. The contact matrix would be:\n\\[\n\\begin{bmatrix}\n40 & 10 \\\\\n90 & 5 \\\\\n\\end{bmatrix}\n\\]\nThese contact matrices have a few cool properties. You can read more about them here.\nBecause of the importance of the student-to-teacher ratio in the example above, contact matrices based on one demographic or social structure are are not generally compatible with other structures. Conveniently, a 2017 study by Kiesha Prem, Alex Cook, and Mark Jit estimated contact matrices for 152 countries, including the United States, using contact surveys and demographic data. We can download their contact matrix for the United States, multiply it by 0.2 to account for the reduced social mixing due to social distancing, and include it as a new parameter, C, in our model. You can view the code for the creation of C and the rest of this analysis at https://github.com/yangjasp/distill_site.\nFor our system of ordinary differential equations we now have:\n\\[\\begin{align*}\n\\frac{dS_i}{dt} &= -\\beta S_i C (I_i/N) + \\omega R_i - \\nu_{1i}  -  \\nu_{2i} \\\\\n\\frac{dV_{1i}}{dt} &=  \\nu_{1i} - (1-\\varphi_1) \\beta V_{1i} C (I_i/N)\\\\\n\\frac{dV_{2i}}{dt} &=  \\nu_{2i} - (1-\\varphi_2) \\beta V_{2i} C (I_i/N)\\\\\n\\frac{dI_i}{dt} &= \\beta S_i I_i - \\gamma I_i + (1-\\varphi_1)\\beta V_{1i} C (I_i/N) + (1-\\varphi_2)\\beta V_{2i} C (I_i/N) \\\\\n\\frac{dR_i}{dt} &= \\gamma I_i(1-\\mu_i) - \\omega R_i\\\\\n\\frac{dD_i}{dt} &= \\gamma I_i(\\mu_i)\n\\end{align*}\\]\nNote that this model assumes that the parameters \\(\\beta\\), \\(\\gamma\\), \\(\\omega\\), \\(\\varphi_1\\), and \\(\\varphi_2\\) are constant across all age classes, while the death rate, \\(\\mu_i\\), and the vaccination rates, \\(\\nu_{1i}\\) and \\(\\nu_{2i}\\), vary across classes."
  },
  {
    "objectID": "posts/vaccine_modelling/index.html#completing-the-final-model",
    "href": "posts/vaccine_modelling/index.html#completing-the-final-model",
    "title": "Modeling COVID-19 and Vaccine Rollout Strategies",
    "section": "Completing the Final Model",
    "text": "Completing the Final Model\nIntroducing a contact matrix allows our model to more accurately capture the rate of transmission across age classes, but it does not account for the fact that asymptomatic carriers are much more likely to come into contact with susceptible individuals than symptomatic ones. With this fact in mind, we introduce one more parameter, \\(\\alpha_i\\), to represent the fraction of asymptomatic infections per age class. This addition should be an important one, as it will help our model better capture the reported role of young adults as viral spreaders. Similarly to the approach described in a 2012 paper by Tower and Feng, we can now split our infected (Is) classes into symptomatic (Is) and asymptomatic (Ia) class. We will also create a new contact matrix, Cs, which for our purposes will be a our old contact matrix, now called Ca, multiplied by some constant \\(k\\) &lt; 1 which represents the fraction of contacts that a symptomatic individual makes compared to an asymptomatic individual on average. This gives us a revised final system of ordinary differential equations:\n\\[\\begin{align*}\n\\frac{dS_i}{dt} &= -\\beta S_i [C^{a} (I^{a}_i/N_i) + C^{s} (I^{s}_i/N_i)] + \\omega R_i - \\nu_{1i}  -  \\nu_{2i} \\\\\n\\frac{dV_{1i}}{dt} &=  \\nu_{1i}  - \\beta (1-\\varphi_{1}) V_{1i} [C^{a} (I_i/N_i) + C^{s}(I^{s}_i/N_i)] \\\\\n\\frac{dV_{2i}}{dt} &=  \\nu_{2i}  - \\beta (1-\\varphi_{2}) V_{2i} [C^{a} (I_i/N_i) + C^{s}(I^{s}_i/N_i)]\\\\\n\\frac{dI^{a}_i}{dt} &=\n\\alpha_i\\Big[\\beta S_i [C^{a} (I^{a}_i/N_i) + C^{s} (I^{s}_i/N_i)] + \\\\ &\\phantom{=}\n\\beta (1-\\varphi_{1}) V_{1i} [C^{a} (I_i/N_i) + C^{s}(I^{s}_i/N_i)] +  \\\\ &\\phantom{=}  \\beta (1-\\varphi_{2}) V_{2i} [C^{a} (I_i/N_i) + C^{s}(I^{s}_i/N_i)]\\Big] -\n\\gamma I^{a}_i \\\\\n\\frac{dI^{s}_i}{dt} &= (1-\\alpha_i)\\Big[\\beta S_i [C^{a} (I^{a}_i/N_i) + C^{s} (I^{s}_i/N_i)] + \\\\ &\\phantom{=}\n\\beta (1-\\varphi_{1}) V_{1i} [C^{a} (I_i/N_i) + C^{s}(I^{s}_i/N_i)] +  \\\\ &\\phantom{=}  \\beta (1-\\varphi_{2}) V_{2i} [C^{a} (I_i/N_i) + C^{s}(I^{s}_i/N_i)]\\Big] -\n\\gamma I^{s}_i \\\\\n\\frac{dR_i}{dt} &= (1-\\mu_i)(\\gamma I^{a}_i + \\gamma I^{s}_i) - \\omega R_i\\\\\n\\frac{dD_i}{dt} &= \\mu_i(\\gamma I^{a}_i + \\gamma I^{s}_i)\n\\end{align*}\\]\nWhere \\(C^{s} = k C^{a}\\) in our case (it doesn’t have to in all cases, clearly, but we will use this for simplicity). This is our final model."
  },
  {
    "objectID": "posts/vaccine_modelling/index.html#running-the-model",
    "href": "posts/vaccine_modelling/index.html#running-the-model",
    "title": "Modeling COVID-19 and Vaccine Rollout Strategies",
    "section": "Running the Model",
    "text": "Running the Model\nWe can now use this model to answer our original question about the rollout of vaccines with different efficacies by adjusting the rates at which they are given in each age class, \\(\\nu_{1i}\\) and \\(\\nu_{2i}\\).\nSo how do we set \\(\\nu_{1i}\\) and \\(\\nu_{2i}\\)? According to a recent Bloomberg article, approximately 200,000 people per day are being vaccinated against COVID-19 on average. This number is pretty low, and some experts have said that this number needs to be much larger if the U.S. wants to reach its goals in 2021. Let’s say that the average number of vaccinations per day increases to 1 million in the new year. Since we defined \\(\\nu_{1i}\\) and \\(\\nu_{2i}\\) as the number of susceptible patients vaccinated per day, we can set a limit on the total vaccinations per day, \\(1,000,000 = \\sum_{i=1}^{18} \\nu_{1i}+\\nu_{2i}\\). Here, we assume that the limiting factor in doses per day is the administration of them, not the availability of the vaccines themselves. Right now, this appears to be a more than reasonable assumption.\nNote: for the models that follow, I will be using the following parameters:\n\n\n\nParameter\nValue\n\n\n\n\n\\(\\beta\\)\n0.05\n\n\n\\(\\gamma\\)\n1/7\n\n\n\\(\\omega\\)\n1/91\n\n\n\\(\\varphi_1\\)\n0.95\n\n\n\\(\\varphi_2\\)\n0.70\n\n\n\\(k\\)\n0.20\n\n\n\n\nScenario 1: Only One Vaccine Available\nLet’s start by supposing that only one vaccine efficacy, 95%, is available. Remembering our discussion about vaccine efficacy, this means that compared to people who are not vaccinated, those who have been vaccinated have 0.05 times the risk of getting COVID. Assuming vaccinated individuals are making the same social contacts as they would if they were not vaccinated, this means that we have \\(\\sum_{i=1}^{18} \\nu_{1i} = 1,000,000\\) (\\(\\nu_{2i} = 0\\) for all \\(i\\)), and \\(\\varphi_{1i} = 0.95\\). That is, people who receive a vaccine receive vaccine 1 and are \\(1-\\varphi_{1i}=0.05\\) times as likely to get COVID as those in the S group.\nBelow is a series of graphs comparing the the overall trajectory of deaths due to COVID-19 in the U.S. with no vaccine, Vaccine 2 (70% Efficacy) alone, and Vaccine 1 (95% Efficacy) alone. In these scenarios, both vaccines are administered starting with the oldest individuals and gradually moving down age classes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe sizeable difference between the three cases is clear and demonstrates the impact that vaccines can have on the COVID-19 pandemic. Both vaccines cut the cumulative death total for 2021 in half and decreased the rate of deaths significantly. If we have an effective vaccine, the rate of COVID deaths starts to slow in late spring, even without a change in our distancing behavior.\nOkay, the results of this scenario were probably pretty obvious. Now for the fun in scenario 2!\n\n\nScenario 2: Vaccines of Different Efficacies\nNow let’s suppose that there are two vaccines of different efficacies available. For the approximately 320 million people in the US, we have 150 doses ordered each for Vaccine 1 (95% efficacy) and Vaccine 2 (70% efficacy). In this scenario, we are still limited to 1 million doses per day, but the vaccines can each only be produced at a rate of 500,000 per day (note that if 1 million per day of the 95% could be produced, we would clearly use that on the most vulnerable individuals until we ran out before starting to produce the 70% and using that on whomever remained). We now have the dilemna that inspired this post!\nClearly we want to vaccinate the oldest and most vulnerable individuals first, but is it better to give them the Vaccine 2 if it means they get vaccinated faster? Let’s consider a couple of approaches:\n\nApproach A : Every individual 65 or older will receive Vaccine 1. Vaccine 2 will only be given to people below 65, starting with the oldest (most vulnerable) and moving down age groups. Once the amount of susceptible individuals that are 65 or older drops below 500,000, the rest of the vaccine 1 doses can be allocated to whoever the oldest individuals left are.\nApproach B: The oldest individuals will be vaccinated as soon as possible with either vaccine. Every day, the oldest 500,000 people will receive Vaccine 1 and the next oldest 500,000 will receive Vaccine 2.\n\n\n\n\n\n\nBased on our model, approach A will save approximately 200,000 more lives overall than approach B. Almost 300,000 lives are predicted to be saved in the age 65+ category, although approach 1 results in more young deaths. This finding suggests that if our goal is to save the most lives, vulnerable individuals need to be vaccinated with an effective vaccine, even if that means it is done at a slower rate.\nBut is it really this simple? Remember that the figure above is based on a Vaccine 1 with 95% efficacy and a Vaccine 2 with 70% efficacy. But what if the difference in efficacies is smaller? The figure below compares the two approaches when Vaccine 2 has an 90% efficacy.\n\n\n\n\n\nUnder these conditions, approach B saves more lives overall. This finding is logical when we consider that approach B would clearly be optimal if both Vaccine 1 and Vaccine 2 had the same efficacy. Putting of vaccinations of vulnerable individuals is not worth it if the two vaccines have similar efficacies.\nSo at what Vaccine 2 efficacy does approach B overcome approach A as optimal? At this point of interest, the impact of a less effective vaccine is approximately equal to the impact of a slower rate of administration. According to our model, the two approaches result in an approximately equal number of deaths when Vaccine 2 has an 85% efficacy. Below 85%, more lives are saved with approach A, but approach B leads to more lives saved when Vaccine 2 has an efficacy above 85%."
  },
  {
    "objectID": "posts/vaccine_modelling/index.html#conclusion",
    "href": "posts/vaccine_modelling/index.html#conclusion",
    "title": "Modeling COVID-19 and Vaccine Rollout Strategies",
    "section": "Conclusion",
    "text": "Conclusion\nWhile the specific numbers predicted by our model will almost certainly vary from the real-life course of COVID-19 in the U.S. in 2021, the most important takeaway from this post is the vital role that vaccine efficacy and rollout strategies will play in determining the pandemic’s course. If and when multiple vaccines become available, policy makers must keep the importance of vaccine efficacy differences in mind as they seek to develop vaccine allocation approaches that maximize lives saved. If a new vaccine has a significantly lower efficacy (&lt; 85% according to our model) compared to the 95% efficacy of Pfizer and Moderna’s, it will be optimal to start administering it to low-risk individuals right away. If the efficacy is similar to 95% (&gt; 85% according to our model), the best strategy is to use it to ensure that the oldest individuals get vaccinated as soon as possible."
  },
  {
    "objectID": "posts/vaccine_modelling/index.html#model-extensions",
    "href": "posts/vaccine_modelling/index.html#model-extensions",
    "title": "Modeling COVID-19 and Vaccine Rollout Strategies",
    "section": "Model Extensions",
    "text": "Model Extensions\nThe model framework set up in this post has the potential to become even more reflective of the real-life pandemic with some extra considerations. The true contact matrix and related transmission rate \\(\\beta\\) are changing constantly as a result of developing societal perceptions of the virus, individual behaviors, and government policies. A time-dependent contact matrix and \\(\\beta_t\\) would allow the model to reflect some of these dymanics. Another possible extension of the model could include more demographic classes that take into account other factors besides age such as job or location.\nAs a final point, this model is deterministic in that its projections are based on exact parameters without accounting for randomness. Despite this limitation, deterministic models are still used widely throughout epidemiology and have had some important applications. I am hoping to play around with a stochastic model that incorporates some of these other ideas in the future!\nAlso, let me know if there are any other scenarios I should explore. I am hoping to make a Shiny app that makes any scenario possible!"
  },
  {
    "objectID": "posts/vaccine_modelling/index.html#appendix-notes-on-parameters-and-initial-conditions",
    "href": "posts/vaccine_modelling/index.html#appendix-notes-on-parameters-and-initial-conditions",
    "title": "Modeling COVID-19 and Vaccine Rollout Strategies",
    "section": "Appendix: Notes on Parameters and Initial Conditions",
    "text": "Appendix: Notes on Parameters and Initial Conditions\n\nThe rate of death once infected with COVID in each of the following age groups is (taken as (deaths per 100,000)/(incidence per 100,000) from CDC estimates. Assumes number of patients re-infected is minimal.):\nThe initial conditions for the model were estimated from Statistica population data and New York Times COVID-19 data as of January 3, 2021. They were:\n\n\nThe models also assumes that:\n\nThe death and infection rates published in the CDC ACIP Report on 12/20/20 are accurate and uniform within their defined age groups.\nThe vaccine is effective at preventing illness and transmission.\nThe contacts made by individuals are reduced by a constant multiplier as a result of social distancing guidelines. The nature and proportional allocation among age classes does not change. This is probably not true, but I am not aware of studies that have quantified the adjusted contact rates.\nNone of the parameters change over time. This is not true, as I assume/hope more extreme measures would be taken to slow the growth if it exploded.\nStochastic effects do not affect the course of the pandemic."
  },
  {
    "objectID": "posts/vaccine_modelling/index.html#references",
    "href": "posts/vaccine_modelling/index.html#references",
    "title": "Modeling COVID-19 and Vaccine Rollout Strategies",
    "section": "References",
    "text": "References\n\nMorris SE (2020). shinySIR: Interactive Plotting for Mathematical Models of Infectious Disease Spread. R package version 0.1.2, https://github.com/SineadMorris/shinySIR.\nPrem K, Cook AR, Jit M (2017) Projecting social contact matrices in 152 countries using contact surveys and demographic data.\nTowers, S. (2012, December 11). SIR Model with Age Classes. Retrieved January 02, 2021, from http://sherrytowers.com/2012/12/11/sir-model-with-age-classes/\nTowers, S., & Feng, Z. (2012). Social contact patterns and control strategies for influenza in the elderly. Mathematical biosciences, 240(2), 241-249.\nComputing Note: All analysis was performed in R statistical software. The package deSolve was used to solve each system of differential equations. Plots were created with ggplot2. All code used in this analysis is available here."
  },
  {
    "objectID": "posts/covid19_bayes/index.html",
    "href": "posts/covid19_bayes/index.html",
    "title": "COVID-19 Testing and Bayes’ Theorem",
    "section": "",
    "text": "In the nine months since COVID-19 diagnostic tests were first made available to the public in March, they have served valiantly as one of the world’s greatest tools for tracking the sweeping spread of the disease. Widespread testing has also enabled the return of professional sports, made the partial re-opening of many college campuses possible, and provided a sense of safety for small social gatherings.\nBut just how reliable are they? The answer to this question carries much more weight at the level of the individual in that a few inaccurate results out of 100 won’t change much in terms of health officials’ overall tracking of the disease, but it could be the difference between life and death of a loved one for the few that the error affects. Studies have suggested the true error rate is probably somewhere around this “few in a hundred” mark, but the accuracy can vary depending on other factors such as the type of test and time since exposure. For this reason, CDC guidelines clearly state that a negative test doesn’t clear someone from the possibility of being infected with COVID, but many people are still willing to act like it does. In some cases, this may be a reasonable risk to take, but a more thorough assessment of the risks requires an understanding of the importance of prior probability."
  },
  {
    "objectID": "posts/covid19_bayes/index.html#bayes-theorem-and-covid-testing",
    "href": "posts/covid19_bayes/index.html#bayes-theorem-and-covid-testing",
    "title": "COVID-19 Testing and Bayes’ Theorem",
    "section": "Bayes’ Theorem and COVID Testing",
    "text": "Bayes’ Theorem and COVID Testing\nReally, a COVID test should be treated as no more than a (quite strong) piece of evidence in a larger pool of information. If person A, who had a lengthy indoor dinner with a symptomatic friend, decides to get tested after experiencing symptoms and gets a positive result, they can be almost certain that they have COVID. If person B, who follows social distancing guidelines stricly, gets a precautionary test that comes back positive, there is still a fair chance that they have COVID, but their probability of being infected is much lower than person A’s. The only difference between these cases is the “other” evidence available in the larger pool, and it matters. Bayes’ Theorem helps us understand why and allows us to quantify the difference. In its simplest form, the theorem can be written as: \\[P(A|B)= \\frac{P(A)*P(B|A)}{P(B)}\\]\nWhere:\n\n\\(P(A|B)\\) is the updated, or posterior, probability of \\(A\\) given the evidence \\(B\\).\n\\(P(A)\\) is the prior probability of \\(A\\) before considering the evidence \\(B\\).\n\\(P(B|A)\\) is the conditional probability of the evidence \\(B\\) occurring supposing that the outcome \\(A\\) is in fact true.\n\\(P(B)\\) is the prior probability of the evidence \\(B\\) occurring before it actually occurred. This is the “universe” of our equation, which we are dividing to take only the portion where \\(A\\) is true.\n\nEssentially, Bayes’ theorem tells us how much a new piece of evidence alters the probability we assigned to the outcome before receiving the new evidence. This altered probability is called the posterior probablility, or \\(P(A|B)\\).\nApplying this theorem to our COVID testing case, we have \\[P(COVID^+|test\\:result)= \\frac{P(COVID^+)*P(test\\:result|COVID^+)}{P(test\\: result)}\\]\nWhile it may not look it, Bayes’ Theorem it is actually quite intuitive in this simple case. This becomes especially apparent when we consider the denominator, \\(P(test\\:result)\\), as the sum of two distinct parts. Because we don’t know for certain whether we have COVID, the test result could be false or true, so one part must account for the probability that the the result occurs and we have COVID and the other part for the probability that the result occurs and we don’t have COVID. Accordingly, we can break \\(P(test\\:result)\\) into the sum of \\(P(COVID^+)*P(test\\:result|COVID^+)\\) and \\(P(COVID^-)*P(test\\:result|COVID^-)\\), giving us a new application of Bayes’ Theorem to COVID:\n\\[\\scriptsize P(COVID^+|test\\:result)= \\frac{P(COVID^+)*P(test\\:result|COVID^+)}{P(COVID^+)*p(test\\:result|COVID^+)+P(COVID^-)*P(test\\:result|COVID^-)}\\]\nNow it is clear to see that this theorem simply represents the proportion of total scenarios where we receive a specific test result in which we also have COVID."
  },
  {
    "objectID": "posts/covid19_bayes/index.html#intepreting-covid-test-results-an-example",
    "href": "posts/covid19_bayes/index.html#intepreting-covid-test-results-an-example",
    "title": "COVID-19 Testing and Bayes’ Theorem",
    "section": "Intepreting COVID Test Results: An Example",
    "text": "Intepreting COVID Test Results: An Example\nScenario One: Let’s suppose that you work from home, strictly follow social distancing guidelines, and avoid leaving the house at all costs but live with one roommate who recently traveled out-of-state to visit family. Upon their return, they start to develop a fever and decide to get a COVID test which comes back positive. You isolate from them immediately and decide to get a Rapid Antigen test five days (to increase the chances of an accurate test by allowing the virus to incubate) later. It comes back negative. What is the probability that you have COVID?\nRecalling our previously outlined Bayesian approach to this question, we need to be able to estimate three probabilities:\n\n\\(P(COVID^+)\\): The prior probability of you having COVID prior to the test. Since you never leave the house, it is nearly certain that the only way you could have been infected with the virus was from your roommate. A recent study estimated the secondary transmission rate of COVID-19 within households to be 53%. This estimate is certainly not perfect, but it does’t have to be, so you reason that your prior probability of having COVID before receiving the test result is 0.53. Notice that because you either have COVID or you don’t, \\(P(COVID^-)\\) is 1-0.53 = 0.47.\n\\(P(test\\:result|COVID^+)\\): The probability that you would test negative given that you have COVID. This probability is called the “false negative rate” of the test. Digging into the research, you find a study that estimates a false negative rate of 20% for the specific test that you received (not bad for a rapid test given that the false negative rates of rapid antigen tests are thought to be anywhere from 10%-50% according to a Harvard Medical School Blog Article). Again, this estimate is probably not perfect, but it’s the best that you have.\n\\(P(test\\:result|COVID^-)\\): The probability that you would test negative given that you do not have COVID. This probability is called the “specificity” of the test. The same study that you used to find the false negative rate estimates the true negative rate to be 95%.\n\nPlugging these numbers into Bayes’ Theorem, you have:\n\\[\\small \\begin{split} P(COVID^+|test^-)  & = \\frac{P(COVID^+)*P(test^-|COVID^+)}{P(test^-)} \\\\ & =\n\\frac{P(COVID^+)*P(test^-|COVID^+)}{p(test^-|COVID^+)*P(COVID^+) + p(test^-|COVID^-)*P(COVID^-)} \\\\ & =\n\\frac{(0.53)(0.20)}{(0.20)(0.53) + (0.95)(1-0.53)} \\\\ & =\n0.192\n\\end{split} \\]\nWith this information, you determine that based on the evidence you have, there is still a 19.2% chance that you have COVID.\nScenario 2: Now suppose that your living situation is the same as scenario one, only you do not have a roommate. You can’t imagine that you have the virus given that you never leave the house, but you want to visit your parents for Thanksgiving, so you decide to get a PCR COVID test before leaving just to be safe. To your surprise, it comes back positive. What is the probability that you have COVID?\nFirst, we estimate the probabilities for Bayes’ Theorem:\n\n\\(P(COVID^+)\\): The prior probability of you having COVID prior to the test. Before getting the test, you thought that it was almost impossible for you to have the virus. If you do have it, you reason that you must have picked it up through a surface transmission on a grocery delivery. This type of transmission is known to be rare, and you usually wash your hands after touching any deliverires. You research to find an estimate that 3% of your county is currently infected with COVID and use that to assign your delivery driver a 3% chance of carrying COVID. You then find research that suggests the probability of you getting COVID from touching the same surface as an infected individual is 5%. Thus, you assign a prior probability of (0.05)(0.02) = 0.001, or 0.1%.\n\\(P(test\\:result|COVID^+)\\): In this scenario, this value is the probability that you would test positive given that you have COVID. This probability is called the “sensitivity” of the test. Digging into the research again, you find a study that estimates a false negative rate of 2% for the PCR COVID tests. If the false negative rate is 2%, that means that the true positive rate is 98%. If 2% of people who have COVID return a negative (falsely), the other 98% must positive (accurately).\n\\(P(test\\:result|COVID^-)\\): The probability that you would test positive given that you do not have COVID. This probability is called the “false positive rate” of the test. The same study that you used to find the test sensitivity estimates the true negative rate to be 95%. You find that an estimated false positive rate of 0.5% for PCR tests.\n\nPlugging these numbers into Bayes’ Theorm Formula, you have:\n\\[\\small \\begin{split} P(COVID^+|test^+)  & = \\frac{P(COVID^+)*P(test^+|COVID^+)}{P(test^+)} \\\\ & =\n\\frac{P(COVID^+)*P(test^+|COVID^+)}{p(test^+|COVID^+)*P(COVID^+) + p(test^+|COVID^-)*P(COVID^-)} \\\\ & =\n\\frac{(0.001)(0.98)}{(0.98)(0.001) + (0.005)(0.999)} \\\\ & =\n0.164\n\\end{split} \\]\nWith this information, you determine that based on the evidence you have, there is only a 16.4% chance that you have COVID.\nI made a simple app using RShiny that calculates these probabilities based on adjustable inputs. Check it out here."
  },
  {
    "objectID": "posts/covid19_bayes/index.html#another-interesting-bayesian-message",
    "href": "posts/covid19_bayes/index.html#another-interesting-bayesian-message",
    "title": "COVID-19 Testing and Bayes’ Theorem",
    "section": "Another Interesting Bayesian Message",
    "text": "Another Interesting Bayesian Message\nIn scenario 2 above, you conclude that the test result is more likely a false positive than a true positive despite the 0.5% false positive rate. Applying this to a larger scale, let’s suppose that a hospital implements a large-scale testing program where each of their 2,000 employees where each is tested weekly for COVID. They strike a deal with a PCR testing company that produces a test with a reported false-positive rate of 0.1% and a false-negative rate of 3% (note that we don’t have great estimates for the true rates for these tests). The COVID case rate in the area is low, and the previous COVID-monitoring program suggested that very few workers had been infected so far over the course of the pandemic. In other words, their COVID-prevention protocols seem to be working.\nAssuming that each worker takes similar precautions, we can assign each of the 2,000 the same underlying (prior) probability of being infected with COVID each week. Since their protocols have been effective, and other evidence suggests the employees are much more careful than the average person, we approximate that this probability is 0.12%. We now have all the pieces of Bayes’ Theorem, which we can use to estimate that the posterior probability of COVID infection for an employee that tests positive is only 0.371.\n\\[\\small \\begin{split} P(COVID^+|test^+)  & = \\frac{P(COVID^+)*P(test^+|COVID^+)}{P(test^+)} \\\\ & =\n\\frac{P(COVID^+)*P(test^+|COVID^+)}{p(test^+|COVID^+)*P(COVID^+) + p(test^+|COVID^-)*P(COVID^-)} \\\\ & =\n\\frac{(0.0012)(0.97)}{(0.97)(0.002) + (0.0012)(0.998)} \\\\ & =\n0.371\n\\end{split} \\]\nThis result means that almost 2/3 of employees that return positive tests each week do not actually have COVID. Since a positive test, regardless of the prior probability, likely means two weeks away from work for the potentially infected individual and their close contacts, this can have some harmful implications for the hospital’s operations. On the other hand, the increased chance of catching the two true positives may be worth the consequences.\nRegardless, this example illustrates the risk of over-testing a group of individuals that have a very low prior probability of being infected. If the underlying case rate is low, most positives are actually false, even if the false positive rate is small."
  },
  {
    "objectID": "posts/covid19_bayes/index.html#closing-remarks",
    "href": "posts/covid19_bayes/index.html#closing-remarks",
    "title": "COVID-19 Testing and Bayes’ Theorem",
    "section": "Closing Remarks",
    "text": "Closing Remarks\nIn reality, every individual either does or doesn’t have COVID. The probability that we are calculating is simply our best guess for the likelihood that someone has COVID based on the evidence that has been collected on the matter*. This probability can prove to be quite valuable when it is used to informing decision making around the disease. Remember, though, that the cost of spreading COVID is extremely high, so it best practice to stick to the side of caution. If there is even a small chance that you have COVID, quarantine and follow CDC guidelines!\n\n\n*If we wanted to be even more robust in our application of Bayes’ Theorem to COVID, we would use probability densities to capture the uncertainty at play in our determination of priors. Maybe in a future post."
  },
  {
    "objectID": "posts/p_values_as_bets/index.html",
    "href": "posts/p_values_as_bets/index.html",
    "title": "Statistical Significance by Betting: Re-framing the p-value?",
    "section": "",
    "text": "The p-value has long been a staple of statistical hypothesis testing, but it has become an increasingly controversial topic in recent decades. In 2019, an article titled “Scientists rise up against statistical significance”, which was co-signed by over 800 statistical researchers, was published in Nature, bringing the topic to the forefront of conversations across the sciences.\nThis controversy has reasonably become a frequent topic of discussion in undergraduate statistics courses, and I have become intrigued by potential solutions. In this post I write about one of the most interesting ones that combines elements of economics and game theory: the proposition that we begin to think of p-values as bets."
  },
  {
    "objectID": "posts/p_values_as_bets/index.html#the-pros-and-cons-of-statistical-significance",
    "href": "posts/p_values_as_bets/index.html#the-pros-and-cons-of-statistical-significance",
    "title": "Statistical Significance by Betting: Re-framing the p-value?",
    "section": "The Pros and Cons of Statistical Significance",
    "text": "The Pros and Cons of Statistical Significance\nThe p-value itself is not an erroneous concept. The issue is that it is so frequently misused and poorly understood that a far too large portion of published scientific conclusions are misguided. This problem arises from the p-value’s close relation to and another concept that is erroneous in nature: statistical significance. While a p-value is simply the probability that the observed results (or more extreme ones) would have occurred under the null hypothesis, statistical significance is the idea that if a p-value lies under a pre-defined threshold (often 0.05 in the biomedical sciences), then the result is “significant”, otherwise is it not.\nOn the surface, statistical significance seems like a reasonable idea, especially considering that it requires that the threshold for significance is pre-defined to prevent researchers from moving it around to force their hypotheses to be “significant”. It also allows researchers to present their results in a more convincing manner (publishing results that say this drug most likely works doesn’t cut it for many), while not making conclusions completely certain because significant ≠ definitely true.\nUnfortunately, no matter how hard statisticians try to hammer the previous inequality into the minds of their colleagues and readers, significance is frequently interpreted as a definite answer. This manner of thinking stems from our (evolutionarily selected for) human desire for certainty, which causes us to make things more black-and-white than they truly are. Thus, if p = 0.049, we reject the null hypothesis and claim our results to be “significant”, but we can’t draw a meaningful conclusion if p = 0.051 even though the difference between these two p-values is really next to meaningless. On top of that, significance is rarely presented in levels, meaning a test that yields a p-value of 0.001 is frequently interpreted to be just as convincing as a test that yields a p-value of 0.04.\nThe current use of the p-value in research also leads to problems when multiple comparisons are involved. While methods such as the Bonferroni correction are frequently used to address these issues at the experimental level, they are more difficult to avoid on larger scales. For example, if 20 different researchers groups study a hypothesis that is not actually true, we can expect one to conclude that the hypothesis is actually significant based on chance alone. When research publication processes favor studies with “significant” results, they are asking for false hypotheses to be presented and accepted by the scientific community. This demonstrates why many research communities are facing replication crises.\nDespite these concerns, some researchers have defended statistical significance by comparing it to concepts like “beyond reasonable doubt” in courts. They point out that we can never know anything with absolute certainty no matter how much evidence there is, so we must live with threshold of significance that helps us get most things right, even if it means risking some incorrect conclusions simply due to chance.\nUltimately, both of these arguments have merit. A major issue in scientific research is incorrect interpretation of p-values and statistical significance, so better education and presentation is certainly needed. But how much will that really change? If we did decide to forget the concept of significance, is there a better alternative?\nMaybe all we need to do is re-frame it."
  },
  {
    "objectID": "posts/p_values_as_bets/index.html#decisions-as-bets",
    "href": "posts/p_values_as_bets/index.html#decisions-as-bets",
    "title": "Statistical Significance by Betting: Re-framing the p-value?",
    "section": "Decisions as Bets",
    "text": "Decisions as Bets\nAs bestselling author and elite poker player Annie Duke points out, decisions are simply bets on the future. We can never know for sure which choice will result in the better outcome, but we make decisions based on the evidence we have and are willing to put more at stake as our perceived probability of a positive outcome increases. As a simple example, I am willing to drive for an hour to visit my friend because based on my assessments (conscious or subconscious), I am so much more likely to have an enjoyable time with him than I am to get in a car accident on the way that the decision to drive is worth it. Plus, staying home by myself won’t be very enjoyable. In contrast, if the weather forecast suggested heavy snow, I might decide that the the enjoyment I would get out of seeing my friend is no longer worth the risk of getting in a car accident. Every decision we make, including which cereal we want to eat for breakfast, follows this same process (although they are subject to frequent errors in our risk/benefit calculations).\nWhen risk calculations are well thought out, making decisions as bets on the future will maximize our chances of favorable outcomes. We will of course get unlucky sometimes, but in the long run we are increasing our chances of success.\nNow, this begs the question: why aren’t scientists treating their research decisions as bets? By using a uniform p-value threshold for significance, we are severely oversimplifying the decision-making process by:\n\nMaking it difficult for audiences to comprehend the strength of the evidence beyond a binary yes/no.\nEssentially treating every research project as being equally important.\n\nIn later sections, I refer to these points as oversimplifications 1 and 2.\nThe current scientific standard is that we are willing to accept a 5% chance of Type I error (incorrectly rejecting the null hypothesis) for every biomedical study. In terms of the friend-visiting example, that is akin to me deciding that I will only make the drive if there is a 50% chance or less of snow, regardless of whether the friend is a distant acquaintance or my lifelong best friend on his deathbed. Clearly, my willingness to risk driving should depend on the context of who the friend is. Even more, if I had to defend my decision to someone else (say, for example, my worried mother), she would certainly react differently if the chance of snow was 49% compared to 0.01%. Is scientific research any different?\nThe concept of statistical significance takes research findings out of their context. Re-framing p-values as bets may lead to better overall research outcomes by bringing the context of the work into the decision."
  },
  {
    "objectID": "posts/p_values_as_bets/index.html#glenn-shafers-betting-scores",
    "href": "posts/p_values_as_bets/index.html#glenn-shafers-betting-scores",
    "title": "Statistical Significance by Betting: Re-framing the p-value?",
    "section": "Glenn Shafer’s Betting Scores",
    "text": "Glenn Shafer’s Betting Scores\nAs I became more interested in this idea of re-framing the p-value as a bet, I stumbled upon the Rutgers-Royal research group. Their project titled “Game-Theoretic Probability and Finance” has been ongoing for over two decades, and they have published a number of interesting papers online, many of which can be found here. Glenn Shafer, a Mathematical Statistician from The University of Rutgers, is a leader of this group. In this section I summarize some of his basic points from this article that specifically tackle issues with the current use of the p-value. Note that the approaches outlined in this section relate primarily to oversimplification 1 from the previous section: the problem of binary (yes/no) significance. It also helps to address problems with multiple comparisons and can be extended to incorporate many more factors\nThe fundamental principle of Dr. Shafer’s alternative to statistical significance is what he calls a betting score. This is defined as:\n\\[\\frac{S(y)}{\\mathbf{E}_p(S)}\\]\nwhere:\n\n\\(S(y)\\) is the payoff received for y occurring\n\\(\\mathbf{E}_P(S)\\) is the expected value1 of the bet under the null hypothesis (which Shafer calls \\(P\\)), or the “money” put at risk.\nThus, we can call \\(S\\) the bet. \\(S\\) is a (non-negative) function.\n\nSince constant multipliers of the bet do not change the ratio (i.e. the betting score is the same whether $1 or $5 is bet, just as betting $100 vs. $150 on your favorite football does not change the bookmaker’s odds), Shafer assumes that \\(\\mathbf{E}_p(S)\\) = 1 for simplicity. In this case, the betting score is just \\(S(y)\\).\nIn addition to making things simpler, setting \\(\\mathbf{E}_p(S)\\) = 1 also conveniently means that \\(SP\\) is a probability distribution defined by the non-negative bet (\\(S(y)\\)) under the null hypothesis (\\(P\\), which is itself a probability distribution). This distribution defines the alternative to P according to the bet, which can be called Q. By this definition, \\(S(y)\\) = \\(Q(y)/P(y)\\), so Shafer shows us that the betting score is a likelihood ratio of the alternative hypothesis to the null hypothesis.\nUnder this simple framework, researchers may present their results as betting scores against the null hypothesis. In other words, they show how much money a bet for Q with an expected value of $1 under the null hypothesis has actually returned based on the scientific evidence. Clearly, higher payoffs mean that the research has presented stronger evidence that the null hypothesis is incorrect.\nA key added bonus of this betting score method is that successive tests can be assessed simply by multiplying betting scores. That is, the money won from the earlier tests (and no more) is used to “buy” a subsequent bet. Here, the accumulated return can be used to assess the overall evidence, which helps to address multiple comparison issues and the closely related replication crisis.\nSo does Dr. Shafer’s testing by betting proposal eliminate the idea of significance altogether? Not really. No matter how a statistical hypothesis test is framed, a sound one requires a way to evaluate it before it is run given the hypotheses. Under the traditional p-value framework, this is where the idea of statistical power comes in. Recall that power tells us the probability that the test will reject the null hypothesis if it is truly incorrect. Under Shafer’s framework, hypothesis tests are set up beforehand with an implied target, \\(S^{\\ast} = \\exp(\\mathbf{E}_Q(\\ln{S}))\\) (see Dr. Shafer’s paper in the references for details about where this comes from). This value is betting score that is defined by the bet and null hypothesis. If Q is correct, this implied target is the expected betting score. Thus, a good test must have a high implied target (For reference, Shafer points out that a standard test always has an implied target of 20 if \\(\\alpha\\) = 0.05) and a reasonable Q. If these conditions are met, the betting score is a valuable, publishable result regardless of how strong it is."
  },
  {
    "objectID": "posts/p_values_as_bets/index.html#considering-research-impact",
    "href": "posts/p_values_as_bets/index.html#considering-research-impact",
    "title": "Statistical Significance by Betting: Re-framing the p-value?",
    "section": "Considering Research Impact",
    "text": "Considering Research Impact\nWhile Glenn Shafer’s theory of betting scores and implied targets helps to address oversimplification 1, the current framework does not discuss the second oversimplification that the current use of statistical significance suffers from: the issue that every project is essentially treated the same.\nMany statisticians will argue that this is in fact a strength of statistical significance, and there are certainly strong arguments supporting this notion. To highlight how it can sometimes be problematic, though, I present the following examples: suppose that an efficacy trial for a pandemic-saving vaccine is run, but the pressure of time and difficulties enrolling participants in the study decreases its statistical power. The study concludes, and the statisticians analyze the data as the public waits eagerly. Finally, the results are published and show some evidence that the vaccine is effective, but p = 0.08. The research team offers to continue testing, but it would be very costly in addition to having methodological problems. Even though the increased efficacy compared to the placebo was not “statistically significant”, how should the results be interpreted?\nIn contrast, let’s now suppose that a vaccine for a rare, non-fatal disease is being tested in a pandemic-free world. A clinical trial with this somewhat risky vaccine is run and yields the exact same results as the study from the previous example. Again, p = 0.08.\nDespite yielding the same statistical results, the context of these two example studies clearly calls for their results to be interpreted differently by society (the stakeholders in the outcome). While the differences between most research projects are not as stark as the differences in these example scenarios, no two studies have the exact same potential risk/benefit to society. Even though researchers, statisticians, and policy makers certainly consider societal impacts in their current interpretations of research, 1) the all-or-nothing framing of statistical significance abets misjudgments by readers and 2) the way in which the potential impact is incorporated with the results is not standardized, nor is it a part of the actual research process. Instead, the interpretation beyond the p-value is often left to non-technical policymakers. Would it not make more sense to include these valuations as part of the research results? If they were to be included, testing by betting would be a perfect framework by which to incorporate them. Turning back to Shafer’s proposals, the pre-determined potential societal impact could be incorporated into the bet and thus also the implied target."
  },
  {
    "objectID": "posts/p_values_as_bets/index.html#the-freerolling-problem-in-science",
    "href": "posts/p_values_as_bets/index.html#the-freerolling-problem-in-science",
    "title": "Statistical Significance by Betting: Re-framing the p-value?",
    "section": "The Freerolling Problem in Science",
    "text": "The Freerolling Problem in Science\nI had difficult time finding theoretical work on a “betting” model for scientific research that would incorporate the potential societal impact. After some digging, though, I came across a very brief discussion of this issue here by a colleague of Shafer’s at Rutgers, Harry Crane. In this mini-talk he uses the term “freerolling”, a word used in gambling to describe betting with a chance to win money but no chance of losing, to help illustrate why research results are so often misrepresented and how framing results as bets could be useful.\nTaking a step back to the earlier example of my decision to visit my friend, the freerolling issue would occur if a third individual who doesn’t care whether or not I get into a car accident makes the decision for me. In fact, this individual would be rewarded if I made the trip but not punished at all if I didn’t. Researchers, specifically statisticians, often very little at stake in regards to the implication of their results. If they submit “significant” results to a journal, they have a greater chance of being published, improving their reputation, and feeling that the work was meaningful. They clearly have a chance to benefit from the claim of “significance”, but if later evidence disproves their results, they won’t face any major consequences and can save their reputation by calling it an unlucky case of type II error. Meanwhile, the patients/population who had a direct stake in the study results may face more grave consequences.\nIn this sense, publishing “significant” results in scientific research is clearly a freeroll bet. Researchers have a chance at being rewarded for publishing misleading results without any chance of being punished. This incentives practices such as “p-hacking” and data over-analysis. So how do we combat this? One approach would be to force researchers to have a stake in their results by betting on them. This idea differs from Shafer’s betting score theory, which is based on conceptual bets as a way of capturing confidence in results, but could still be incorporated as an extension."
  },
  {
    "objectID": "posts/p_values_as_bets/index.html#conclusion",
    "href": "posts/p_values_as_bets/index.html#conclusion",
    "title": "Statistical Significance by Betting: Re-framing the p-value?",
    "section": "Conclusion",
    "text": "Conclusion\nThe message of this post is not that there is something wrong with p-values. The problem is that the binary, all-or-nothing use of statistical significance that doesn’t take into account the context of the study question is often incorrectly used to interpret them. While Glenn Shafer’s proposed betting score approach combats the issue of binary classification, it doesn’t account for the potential study impact and thus would treat the results from the two examples above the exact same. It seems that these significance by betting theories could possibly be extended to also differentiate between studies with different risk/benefit distributions.\nAs an aspiring statistician, I am fascinated not only by the way in which data is analyzed, but also by the way in which it is interpreted and meaningfully presented. My limited experience in statistical research has shown me the limitations that the concept of statistical significance presents when it comes to this task of presenting data. I wrote this post to document some interesting ideas that I discovered on this topic, but I am not necessarily in favor of all of them. It will be exciting to to see how this conversation develops in the future!"
  },
  {
    "objectID": "posts/p_values_as_bets/index.html#references",
    "href": "posts/p_values_as_bets/index.html#references",
    "title": "Statistical Significance by Betting: Re-framing the p-value?",
    "section": "References",
    "text": "References\n\nCrane, H. (2020). Response to Glenn Shafer’s Testing by betting: A strategy for statistical and scientific communication [presented at the Royal Statistical Society Conference]. https://www.youtube.com/watch?v=qKWF737Av2o\nShafer, G. (2019). The language of betting as a strategy for statistical and scientific communication. arXiv preprint arXiv:1903.06991.\nShafer, G. (2020). Testing by betting: A strategy for statistical and scientific communication [presented at the Royal Statistical Society Conference]. https://www.youtube.com/watch?v=qKWF737Av2o\nShafer, G. (2021). Let’s replace (p-value + power) with (betting outcome + betting target) [presented at NC State University]. https://www.youtube.com/watch?v=rnS08IRubGM"
  },
  {
    "objectID": "posts/p_values_as_bets/index.html#notes",
    "href": "posts/p_values_as_bets/index.html#notes",
    "title": "Statistical Significance by Betting: Re-framing the p-value?",
    "section": "Notes",
    "text": "Notes\n1 Expected Value is the weighted average, or mean, of a random variable. In the context of betting, take a situation where the future has two discrete outcomes. Let’s say that there is a 10% chance that I win $20 and a 90% chance that I lose $1. The expected value of this bet is 0.1*20 + 0.9*-1 = $1.1."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jasper Yang",
    "section": "",
    "text": "I am a second-year PhD student in the Department of Biostatistics at the University of Washington. I graduated from Grinnell College with a major in biology and concentration in statistics before earning an MSc in statistics from the University of Essex. I am currently researching study designs for measurement error correction with the Shaw Lab at Kaiser Permanente Washington Health Research Institute. My graduate studies are supported by the NSF Graduate Research Fellowship.\nMy primary research interests are in measurement error, decision theory, Bayesian inference, and causal inference. I am also interested in rationality and the philosophy of science. I am passionate about problems that seek to find optimal decisions given data and uncertainty, particularly those in biology and medicine.\nYou can reach me on twitter @jyang29 or via email at jbyang@uw.edu. Thanks for visiting!"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Jasper Yang",
    "section": "",
    "text": "I am a second-year PhD student in the Department of Biostatistics at the University of Washington. I graduated from Grinnell College with a major in biology and concentration in statistics before earning an MSc in statistics from the University of Essex. I am currently researching study designs for measurement error correction with the Shaw Lab at Kaiser Permanente Washington Health Research Institute. My graduate studies are supported by the NSF Graduate Research Fellowship.\nMy primary research interests are in measurement error, decision theory, Bayesian inference, and causal inference. I am also interested in rationality and the philosophy of science. I am passionate about problems that seek to find optimal decisions given data and uncertainty, particularly those in biology and medicine.\nYou can reach me on twitter @jyang29 or via email at jbyang@uw.edu. Thanks for visiting!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a second-year PHD student in the Department of Biostatistics at the University of Washington. I graduated from Grinnell College with a major in biology and concentration in statistics before earning an MSc in statistics from the University of Essex. I am currently researching study designs for measurement error correction with the Shaw Lab at Kaiser Permanente Washington Health Research Institute. My graduate studies are supported by the NSF Graduate Research Fellowship.\nMy primary research interests are in measurement error, decision theory, Bayesian inference, and causal inference. I am also interested in rationality and the philosophy of science. I am passionate about problems that seek to find optimal decisions given data and uncertainty, particularly those in biology and medicine.You can reach me on twitter @jyang29 or via email at jbyang@uw.edu. Thanks for visiting!"
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "I am a second-year PHD student in the Department of Biostatistics at the University of Washington. I graduated from Grinnell College with a major in biology and concentration in statistics before earning an MSc in statistics from the University of Essex. I am currently researching study designs for measurement error correction with the Shaw Lab at Kaiser Permanente Washington Health Research Institute. My graduate studies are supported by the NSF Graduate Research Fellowship.\nMy primary research interests are in measurement error, decision theory, Bayesian inference, and causal inference. I am also interested in rationality and the philosophy of science. I am passionate about problems that seek to find optimal decisions given data and uncertainty, particularly those in biology and medicine.You can reach me on twitter @jyang29 or via email at jbyang@uw.edu. Thanks for visiting!"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Rediscovering a fun probability problem\n\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2023\n\n\nJasper Yang\n\n\n\n\n\n\n  \n\n\n\n\nMay the Luckiest Team Win\n\n\n\n\n\n\n\nsports\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\nJasper Yang\n\n\n\n\n\n\n  \n\n\n\n\nStatistical Significance by Betting: Re-framing the p-value?\n\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2021\n\n\nJasper Yang\n\n\n\n\n\n\n  \n\n\n\n\nModeling COVID-19 and Vaccine Rollout Strategies\n\n\n\n\n\n\n\n\n\n\n\n\nDec 28, 2020\n\n\nJasper Yang\n\n\n\n\n\n\n  \n\n\n\n\nCOVID-19 Testing and Bayes’ Theorem\n\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2020\n\n\nJasper Yang\n\n\n\n\n\n\nNo matching items"
  }
]